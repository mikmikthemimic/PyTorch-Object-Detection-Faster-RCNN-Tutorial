{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import logging\n",
    "import json\n",
    "import pathlib\n",
    "from itertools import chain\n",
    "import numpy\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "from metrics.enumerators import MethodAveragePrecision\n",
    "from metrics.pascal_voc_evaluator import (\n",
    "    get_pascalvoc_metrics,\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    from_file_to_boundingbox,\n",
    "    get_filenames_of_path,\n",
    ")\n",
    "\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "# root directory\n",
    "FOLDER_PATH = pathlib.Path(os.getcwd()).parent.absolute()\n",
    "MODULE_PATH = FOLDER_PATH.parent.absolute() / \"PyTorch-Object-Detection-Faster-RCNN-Tutorial\"\n",
    "DATA_PATH = MODULE_PATH / \"pytorch_faster_rcnn_tutorial\" / \"data\" / \"heads\"\n",
    "\n",
    "\n",
    "# input and target files\n",
    "predictions = get_filenames_of_path(DATA_PATH / \"predictions\" / \"GMT3-488\" / \"predictions\")\n",
    "predictions.sort()\n",
    "# test-target files\n",
    "gt_predictions = get_filenames_of_path(DATA_PATH / \"test_targets\")\n",
    "gt_predictions.sort()\n",
    "\n",
    "\n",
    "# get the gt_boxes from disk  || test-targets\n",
    "gt_boxes = [\n",
    "    from_file_to_boundingbox(file_name, groundtruth=True)\n",
    "    for file_name in gt_predictions\n",
    "]\n",
    "# reduce list\n",
    "gt_boxes = list(chain(*gt_boxes))\n",
    "\n",
    "pred_boxes = [\n",
    "    from_file_to_boundingbox(file_name, groundtruth=False)\n",
    "    for file_name in predictions\n",
    "]\n",
    "pred_boxes = list(chain(*pred_boxes))\n",
    "\n",
    "output = get_pascalvoc_metrics(\n",
    "    gt_boxes=gt_boxes,\n",
    "    det_boxes=pred_boxes,\n",
    "    iou_threshold=0.50,\n",
    "    method=MethodAveragePrecision.EVERY_POINT_INTERPOLATION,\n",
    "    generate_table=True,\n",
    ")\n",
    "\n",
    "per_class, m_ap = output[\"per_class\"], output[\"m_ap\"]\n",
    "#print(per_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'det_boxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     classes_bbs[c][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(bb)\n\u001b[0;32m     14\u001b[0m gt_classes_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(gt_classes_only))\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdet_boxes\u001b[49m:\n\u001b[0;32m     16\u001b[0m     c \u001b[38;5;241m=\u001b[39m bb\u001b[38;5;241m.\u001b[39mget_class_id()\n\u001b[0;32m     17\u001b[0m     confidence_score \u001b[38;5;241m=\u001b[39m bb\u001b[38;5;241m.\u001b[39mget_confidence()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'det_boxes' is not defined"
     ]
    }
   ],
   "source": [
    "from metrics.bounding_box import BoundingBox\n",
    "from metrics.enumerators import MethodAveragePrecision\n",
    "\n",
    "ret = {}\n",
    "ctr = 0\n",
    "# Get classes of all bounding boxes separating them by classes\n",
    "gt_classes_only = []\n",
    "classes_bbs = {}\n",
    "for bb in gt_boxes:\n",
    "    c = bb.get_class_id()\n",
    "    gt_classes_only.append(c)\n",
    "    classes_bbs.setdefault(c, {\"gt\": [], \"det\": []})\n",
    "    classes_bbs[c][\"gt\"].append(bb)\n",
    "gt_classes_only = list(set(gt_classes_only))\n",
    "for bb in pred_boxes:\n",
    "    c = bb.get_class_id()\n",
    "    confidence_score = bb.get_confidence()\n",
    "    if c == 'Vehicle' and confidence_score < 0.5:\n",
    "        continue\n",
    "    elif c == 'Pedestrian' and confidence_score < 0.2:\n",
    "        continue\n",
    "    elif c == 'Bicycle' and confidence_score < 0.2:\n",
    "        continue\n",
    "    classes_bbs.setdefault(c, {\"gt\": [], \"det\": []})\n",
    "    classes_bbs[c][\"det\"].append(bb)\n",
    "# Precision x Recall is obtained individually by each class\n",
    "for c, v in classes_bbs.items():\n",
    "    # Report results only in the classes that are in the GT\n",
    "    if c not in gt_classes_only:\n",
    "        continue\n",
    "    npos = len(v[\"gt\"])\n",
    "    # sort detections by decreasing confidence\n",
    "    dects = [\n",
    "        a\n",
    "        for a in sorted(v[\"det\"], key=lambda bb: bb.get_confidence(), reverse=True)\n",
    "    ]\n",
    "    for n, g in enumerate(gt_boxes):\n",
    "        for idx_det, det in enumerate(dects):\n",
    "            iou = BoundingBox.iou(g, det)\n",
    "            if iou == 0 and g:\n",
    "                ctr += 1\n",
    "            else: continue\n",
    "    print(c,ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision <class 'numpy.ndarray'>\n",
      "recall <class 'numpy.ndarray'>\n",
      "AP <class 'numpy.float64'>\n",
      "interpolated precision <class 'list'>\n",
      "interpolated recall <class 'list'>\n",
      "total positives <class 'int'>\n",
      "total tp <class 'numpy.float64'>\n",
      "total fp <class 'numpy.float64'>\n",
      "total fn <class 'numpy.float64'>\n",
      "method <enum 'MethodAveragePrecision'>\n",
      "iou <class 'float'>\n",
      "table <class 'pandas.core.frame.DataFrame'>\n",
      "precision <class 'numpy.ndarray'>\n",
      "recall <class 'numpy.ndarray'>\n",
      "AP <class 'numpy.float64'>\n",
      "interpolated precision <class 'list'>\n",
      "interpolated recall <class 'list'>\n",
      "total positives <class 'int'>\n",
      "total tp <class 'numpy.float64'>\n",
      "total fp <class 'numpy.float64'>\n",
      "total fn <class 'numpy.float64'>\n",
      "method <enum 'MethodAveragePrecision'>\n",
      "iou <class 'float'>\n",
      "table <class 'pandas.core.frame.DataFrame'>\n",
      "precision <class 'numpy.ndarray'>\n",
      "recall <class 'numpy.ndarray'>\n",
      "AP <class 'numpy.float64'>\n",
      "interpolated precision <class 'list'>\n",
      "interpolated recall <class 'list'>\n",
      "total positives <class 'int'>\n",
      "total tp <class 'numpy.float64'>\n",
      "total fp <class 'numpy.float64'>\n",
      "total fn <class 'numpy.float64'>\n",
      "method <enum 'MethodAveragePrecision'>\n",
      "iou <class 'float'>\n",
      "table <class 'pandas.core.frame.DataFrame'>\n",
      "precision <class 'numpy.ndarray'>\n",
      "recall <class 'numpy.ndarray'>\n",
      "AP <class 'numpy.float64'>\n",
      "interpolated precision <class 'list'>\n",
      "interpolated recall <class 'list'>\n",
      "total positives <class 'int'>\n",
      "total tp <class 'numpy.float64'>\n",
      "total fp <class 'numpy.float64'>\n",
      "total fn <class 'numpy.float64'>\n",
      "method <enum 'MethodAveragePrecision'>\n",
      "iou <class 'float'>\n",
      "table <class 'pandas.core.frame.DataFrame'>\n",
      "precision <class 'numpy.ndarray'>\n",
      "recall <class 'numpy.ndarray'>\n",
      "AP <class 'numpy.float64'>\n",
      "interpolated precision <class 'list'>\n",
      "interpolated recall <class 'list'>\n",
      "total positives <class 'int'>\n",
      "total tp <class 'numpy.float64'>\n",
      "total fp <class 'numpy.float64'>\n",
      "total fn <class 'numpy.float64'>\n",
      "method <enum 'MethodAveragePrecision'>\n",
      "iou <class 'float'>\n",
      "table <class 'pandas.core.frame.DataFrame'>\n",
      "precision <class 'numpy.ndarray'>\n",
      "recall <class 'numpy.ndarray'>\n",
      "AP <class 'numpy.float64'>\n",
      "interpolated precision <class 'list'>\n",
      "interpolated recall <class 'list'>\n",
      "total positives <class 'int'>\n",
      "total tp <class 'numpy.float64'>\n",
      "total fp <class 'numpy.float64'>\n",
      "total fn <class 'numpy.float64'>\n",
      "method <enum 'MethodAveragePrecision'>\n",
      "iou <class 'float'>\n",
      "table <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Pedestrian\", \"Pedestrian-Violator\", \"Vehicle\", \"Vehicle-Violator\", \"Bicycle\", \"Bicycle-Violator\"]\n",
    "\n",
    "for label in labels:\n",
    "    head = per_class[label]\n",
    "    for key, value in head.items():\n",
    "        print(key, type(value))\n",
    "        if isinstance(value, numpy.ndarray):\n",
    "            head[key] = value.tolist()\n",
    "        if isinstance(value, MethodAveragePrecision):\n",
    "            head[key] = str(value)\n",
    "        if isinstance(value, pandas.core.frame.DataFrame):\n",
    "            head[key] = value.to_dict()\n",
    "        if isinstance(value, numpy.int32):\n",
    "            head[key] = int(value)\n",
    "\n",
    "    with open(f'my ground truths/{label}.json', 'w') as f:\n",
    "        json.dump(head, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
