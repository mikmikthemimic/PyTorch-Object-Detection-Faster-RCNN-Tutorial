{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb32a80-0e1b-4ddf-b2ff-1fe20611345b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e496b5c5-b8f1-40b3-821c-ed62af8f54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ast\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import neptune\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "from pytorch_faster_rcnn_tutorial.backbone_resnet import ResNetBackbones\n",
    "from pytorch_faster_rcnn_tutorial.datasets import (\n",
    "    ObjectDetectionDataSet,\n",
    "    ObjectDetectionDatasetSingle,\n",
    ")\n",
    "from pytorch_faster_rcnn_tutorial.faster_RCNN import get_faster_rcnn_resnet\n",
    "from pytorch_faster_rcnn_tutorial.transformations import (\n",
    "    ComposeDouble,\n",
    "    ComposeSingle,\n",
    "    FunctionWrapperDouble,\n",
    "    FunctionWrapperSingle,\n",
    "    apply_nms,\n",
    "    apply_score_threshold,\n",
    "    normalize_01,\n",
    ")\n",
    "from pytorch_faster_rcnn_tutorial.utils import (\n",
    "    collate_single,\n",
    "    get_filenames_of_path,\n",
    "    save_json,\n",
    ")\n",
    "from pytorch_faster_rcnn_tutorial.viewers.object_detection_viewer import (\n",
    "    ObjectDetectionViewer,\n",
    "    ObjectDetectionViewerSingle,\n",
    ")\n",
    "from training_script import NeptuneSettings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f582ef-cf8b-4ddd-bfce-e986bfa6a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "params = {\n",
    "    \"EXPERIMENT\": \"GMT3-357\",  # experiment name, e.g. Head-42\n",
    "    \"OWNER\": \"mikmikthemimic\",  # e.g. johndoe55\n",
    "    \"INPUT_DIR\": \"pytorch_faster_rcnn_tutorial/data/heads/test\",  # files to predict\n",
    "    \"PREDICTIONS_PATH\": \"predictions\",  # where to save the predictions\n",
    "    \"MODEL_DIR\": \"model/kfolds/best_model.pt\",  # load model from checkpoint\n",
    "    \"DOWNLOAD\": True,  # whether to download from neptune\n",
    "    \"DOWNLOAD_PATH\": \"model\",  # where to save the model if DOWNLOAD is True\n",
    "    \"PROJECT\": \"GM-Thesis3\",  # Project name\n",
    "\n",
    "    \"ENSEMBLE\": False,  # nilagay ko lang to for the ensemble model\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f8bab4-464b-480b-883e-3e8e5e267ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 640 files in pytorch_faster_rcnn_tutorial/data/heads/test\n"
     ]
    }
   ],
   "source": [
    "# input files\n",
    "inputs = get_filenames_of_path(pathlib.Path(params[\"INPUT_DIR\"]))\n",
    "inputs.sort()\n",
    "print(f\"Found {len(inputs)} files in {params['INPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c30ad2-9fd9-4090-b300-78a36fe83ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "transforms = ComposeSingle(\n",
    "    [\n",
    "        FunctionWrapperSingle(np.moveaxis, source=-1, destination=0),\n",
    "        FunctionWrapperSingle(normalize_01),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f296e5-971c-40bc-a7ae-cdb3f110ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = ObjectDetectionDatasetSingle(\n",
    "    inputs=inputs,\n",
    "    transform=transforms,\n",
    "    use_cache=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e02b562-62e3-42a1-abbd-892c3b77f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "dataloader_prediction = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_single,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe76faa4-35aa-4851-8a6d-91f601f425f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment variables (pydantic BaseSettings class)\n",
    "neptune_settings: NeptuneSettings = NeptuneSettings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "618796d3-1009-42ce-8fbc-4e8197a652ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: mikmikthemimic/GM-Thesis3\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/mikmikthemimic/GM-Thesis3/e/GMT3-357\n"
     ]
    }
   ],
   "source": [
    "# import experiment from neptune\n",
    "project_name = f'{params[\"OWNER\"]}/{params[\"PROJECT\"]}'\n",
    "print(f\"Project: {project_name}\")\n",
    "project = neptune.init_run(\n",
    "    project=project_name,\n",
    "    api_token=neptune_settings.api_key,\n",
    "    with_id=params[\"EXPERIMENT\"],\n",
    "    mode=\"read-only\",\n",
    ")  # get project\n",
    "experiment_id = params[\"EXPERIMENT\"]  # experiment id\n",
    "parameters = project['training/hyperparams'].fetch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3535b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACCELERATOR': 'cuda', 'ANCHOR_SIZE': '((32,), (64,), (128,), (256,))', 'ASPECT_RATIOS': '((0.5, 1.0, 2.0),)', 'BACKBONE': 'ResNetBackbones.RESNET34', 'BATCH_SIZE': 6, 'CACHE': True, 'CLASSES': 4, 'FAST_DEV_RUN': False, 'FOLDS': 5, 'FPN': True, 'IMG_MEAN': '[0.485, 0.456, 0.406]', 'IMG_STD': '[0.229, 0.224, 0.225]', 'IOU_THRESHOLD': 0.6, 'LOG_MODEL': True, 'LR': 0.002, 'MAXEPOCHS': 20, 'MAX_SIZE': 1025, 'MIN_SIZE': 1024, 'PATIENCE': 50, 'PRECISION': 32, 'SAVE_DIR': '/content/PyTorch-Object-Detection-Faster-RCNN-Tutorial', 'SEED': 42, 'iou_threshold': 0.6, 'lr': 0.002, 'model': 'None'}\n"
     ]
    }
   ],
   "source": [
    "print(project['training/hyperparams'].fetch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f455a449-db53-4e14-aa41-e2192ce719a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcnn transform\n",
    "transform = GeneralizedRCNNTransform(\n",
    "    min_size=int(parameters[\"MIN_SIZE\"]),\n",
    "    max_size=int(parameters[\"MAX_SIZE\"]),\n",
    "    image_mean=ast.literal_eval(parameters[\"IMG_MEAN\"]),\n",
    "    image_std=ast.literal_eval(parameters[\"IMG_STD\"]),\n",
    "    box_nms_thresh = 0.6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ada9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color mapping\n",
    "color_mapping = {\n",
    "    1: \"blue\",\n",
    "    2: \"green\",\n",
    "    3: \"white\",\n",
    "    4: \"yellow\",\n",
    "    5: \"red\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8277d891-0926-4005-938f-a8caf9646ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model from neptune or load from checkpoint\n",
    "if params[\"DOWNLOAD\"]:\n",
    "    download_path = pathlib.Path(os.getcwd()) / params[\"DOWNLOAD_PATH\"]\n",
    "    download_path.mkdir(parents=True, exist_ok=True)\n",
    "    if params[\"ENSEMBLE\"]:\n",
    "        model_name = \"ensemble_model.pt\"  # that's how I called the best model\n",
    "        # model_name = properties['checkpoint_name']  # logged when called log_model_neptune()\n",
    "        if not (download_path / model_name).is_file():\n",
    "            project['artifacts/ensemble_model'].download(\n",
    "                destination=download_path.as_posix()\n",
    "            )  # download model\n",
    "\n",
    "        model_state_dict = torch.load(\n",
    "            download_path / model_name, map_location=torch.device(\"cpu\")\n",
    "        )\n",
    "    else:\n",
    "        model_name = \"best_model.pt\"  # that's how I called the best model\n",
    "        # model_name = properties['checkpoint_name']  # logged when called log_model_neptune()\n",
    "        if not (download_path / model_name).is_file():\n",
    "            project['artifacts/best_model'].download(\n",
    "                destination=download_path.as_posix()\n",
    "            )  # download model\n",
    "\n",
    "        model_state_dict = torch.load(\n",
    "            download_path / model_name, map_location=torch.device(\"cpu\")\n",
    "        )\n",
    "else:\n",
    "    checkpoint = torch.load(params[\"MODEL_DIR\"], map_location=torch.device(\"cpu\"))\n",
    "    model_state_dict = checkpoint[\"hyper_parameters\"][\"model\"].state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a78b32-2283-45ad-83c2-11337e4c2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abiga\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\abiga\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = get_faster_rcnn_resnet(\n",
    "    num_classes=int(parameters[\"CLASSES\"]),\n",
    "    backbone_name=ResNetBackbones(parameters[\"BACKBONE\"].split(\".\")[1].lower()),  # reverse look-up enum\n",
    "    anchor_size=ast.literal_eval(parameters[\"ANCHOR_SIZE\"]),\n",
    "    aspect_ratios=ast.literal_eval(parameters[\"ASPECT_RATIOS\"]),\n",
    "    fpn=ast.literal_eval(str(parameters['FPN'])),\n",
    "    min_size=int(parameters[\"MIN_SIZE\"]),\n",
    "    max_size=int(parameters[\"MAX_SIZE\"]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9098dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if keys start with model. and remove it\n",
    "model_state_dict = {k[6:]: v for k, v in model_state_dict.items() if k.startswith(\"model.\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d659f25-0d7d-4a97-be15-5795e24f2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e59d69e-b9ca-45b3-8320-2a7d4a0e4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference (cpu)\n",
    "model.eval()\n",
    "for sample in dataloader_prediction:\n",
    "    x, x_name = sample\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)\n",
    "        pred = {key: value.numpy() for key, value in pred[0].items()}\n",
    "        name = pathlib.Path(x_name[0])\n",
    "        save_dir = pathlib.Path(os.getcwd()) / params[\"PREDICTIONS_PATH\"]\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        pred_list = {\n",
    "            key: value.tolist() for key, value in pred.items()\n",
    "        }  # numpy arrays are not serializable -> .tolist()\n",
    "        save_json(pred_list, path=save_dir / name.with_suffix(\".json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6c30a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction files\n",
    "predictions = get_filenames_of_path(\n",
    "    pathlib.Path(os.getcwd()) / params[\"PREDICTIONS_PATH\"]\n",
    ")\n",
    "predictions.sort()\n",
    "\n",
    "# create prediction dataset\n",
    "iou_threshold = 0.5\n",
    "score_threshold = 0.7\n",
    "\n",
    "transforms_prediction = ComposeDouble(\n",
    "    [\n",
    "        FunctionWrapperDouble(np.moveaxis, source=-1, destination=0),\n",
    "        FunctionWrapperDouble(normalize_01),\n",
    "        FunctionWrapperDouble(\n",
    "            apply_nms, input=False, target=True, iou_threshold=iou_threshold\n",
    "        ),\n",
    "        FunctionWrapperDouble(\n",
    "            apply_score_threshold,\n",
    "            input=False,\n",
    "            target=True,\n",
    "            score_threshold=score_threshold,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_prediction = ObjectDetectionDataSet(\n",
    "    inputs=inputs, targets=predictions, transform=transforms_prediction, use_cache=False, convert_to_format=\"xywh\"\n",
    ")\n",
    "\n",
    "color_mapping = {\n",
    "    1: \"blue\",\n",
    "    2: \"green\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72f312d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions\n",
    "datasetviewer_prediction = ObjectDetectionViewer(\n",
    "    dataset=dataset_prediction, color_mapping=color_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d1819e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
